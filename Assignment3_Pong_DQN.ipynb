{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92fd4d6f",
   "metadata": {},
   "source": [
    "# CSCN8020 – Assignment 3  \n",
    "## Deep Q-Learning on Atari Pong (TensorFlow Version)\n",
    "\n",
    "**Student Name:** Haysam Elamin  \n",
    "**Student ID:** 8953681  \n",
    "\n",
    "This notebook implements a Deep Q-Network (DQN) agent to play **Atari Pong**,\n",
    "using **Gymnasium** and **TensorFlow/Keras**.\n",
    "\n",
    "It also connects conceptually to the **Multi-Armed Bandit / Exploration Strategies**\n",
    "workshop, explored ε-greedy, stationary and non-stationary\n",
    "casinos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2a2d07",
   "metadata": {},
   "source": [
    "## Notebook Outline\n",
    "\n",
    "1. Environment & Library Setup  \n",
    "2. Pong Environment Overview  \n",
    "3. Frame Preprocessing (Cropping, Downsampling, Grayscale, Normalization)  \n",
    "4. DQN Agent (TensorFlow/Keras) – Architecture & Replay Buffer  \n",
    "5. Training Loop (ε-greedy, Experience Replay, Target Network)  \n",
    "6. Results & Visualizations (Rewards over Episodes, Example Frames)  \n",
    "7. Reflections: Exploration vs Exploitation & Link to Casino Exercise  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762f6e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Environment & Library Setup\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym  # Newer version of gym\n",
    "import tensorflow as tf\n",
    "\n",
    "from assignment3_utility import process_frame, transform_reward\n",
    "from assignment3_agent_tf import DQNAgent\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Ensure reproducibility (as much as possible)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966e5b1a",
   "metadata": {},
   "source": [
    "## 2. Pong Environment Overview\n",
    "\n",
    "We use the **`ALE/Pong-v5`** environment from Gymnasium, which simulates the classic Atari Pong game.\n",
    "\n",
    "- **Observation:** RGB frames of size 210×160×3.  \n",
    "- **Actions:** Discrete actions such as:\n",
    "  - 0: NOOP (no operation)\n",
    "  - 2: Move paddle **up**\n",
    "  - 3: Move paddle **down**\n",
    "\n",
    "We will wrap the environment so that:\n",
    "\n",
    "- Frames are preprocessed using our `process_frame` function.  \n",
    "- We apply the new Gymnasium API (`reset()` returns `(obs, info)` and `step()` returns `(obs, reward, terminated, truncated, info)`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baee5258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pong environment\n",
    "################# Note: If ALE/Pong-v5 is not installed, you may need:\n",
    "############## pip install \"gymnasium[atari,accept-rom-license]\"\n",
    "env = gym.make(\"ALE/Pong-v5\", render_mode=None)\n",
    "\n",
    "obs, info = env.reset()\n",
    "print(\"Raw observation shape:\", obs.shape)\n",
    "print(\"Action space:\", env.action_space)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf7a835",
   "metadata": {},
   "source": [
    "## 3. Frame Preprocessing\n",
    "\n",
    "The raw Pong frames are large and contain many irrelevant details (score, borders, backgrounds).\n",
    "\n",
    "We apply the following steps:\n",
    "\n",
    "1. **Crop** the top and bottom to remove borders and HUD.  \n",
    "2. **Downsample** by 2 to reduce resolution and speed up training.  \n",
    "3. Convert to **grayscale** (1 channel instead of 3).  \n",
    "4. **Normalize** to the range [-1, 1] to stabilize neural network training.  \n",
    "5. Reshape to `(1, H, W, 1)` so it can be fed into a Keras CNN.\n",
    "\n",
    "These steps are implemented in `assignment3_utility.py` in the `process_frame()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb27219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test frame preprocessing\n",
    "IMAGE_SHAPE = (80, 80)  # desired (H, W)\n",
    "\n",
    "processed = process_frame(obs, IMAGE_SHAPE)\n",
    "print(\"Processed frame shape:\", processed.shape)  # Expect (1, 80, 80, 1)\n",
    "\n",
    "# Visualize original vs processed ( to see the effect)\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Pong Frame\")\n",
    "plt.imshow(obs)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Processed Grayscale Frame\")\n",
    "plt.imshow(processed[0, :, :, 0], cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c13d43",
   "metadata": {},
   "source": [
    "## 4. DQN Agent (TensorFlow/Keras)\n",
    "\n",
    "We implement a **Deep Q-Network (DQN)** agent, which uses:\n",
    "\n",
    "- A **Convolutional Neural Network (CNN)** to map images → Q-values.\n",
    "- An **experience replay buffer** to break correlation between consecutive samples.\n",
    "- A **target network** to stabilize learning.\n",
    "- An **ε-greedy policy** to balance exploration vs exploitation:\n",
    "  - Start with ε = 1.0 (mostly random actions).\n",
    "  - Gradually decay ε to 0.1 over 1 million frames.\n",
    "\n",
    "The implementation is located in `assignment3_agent_tf.py` and exposes:\n",
    "\n",
    "- `DQNAgent` class\n",
    "- `select_action(state)` for training (ε-greedy)\n",
    "- `select_greedy_action(state)` for evaluation\n",
    "- `train_step()` to perform one gradient update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1108b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine state shape for the agent\n",
    "state_shape = (IMAGE_SHAPE[0], IMAGE_SHAPE[1], 1)  # (H, W, C)\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "agent = DQNAgent(\n",
    "    state_shape=state_shape,\n",
    "    num_actions=num_actions,\n",
    "    gamma=0.99,\n",
    "    buffer_capacity=100_000,\n",
    "    batch_size=32,\n",
    "    min_replay_size=10_000,      # start learning after collecting enough data\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.1,\n",
    "    epsilon_decay_frames=1_000_000,\n",
    "    target_update_freq=10_000\n",
    ")\n",
    "\n",
    "print(\"Agent initialized with state shape:\", state_shape)\n",
    "print(\"Number of actions:\", num_actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aeb410",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n",
    "\n",
    "The training loop performs the following steps per episode:\n",
    "\n",
    "1. **Reset** the environment and preprocess the first frame as the initial state.  \n",
    "2. For each time step:\n",
    "   - Select an action using the agent's **ε-greedy policy**.\n",
    "   - Apply the action using `env.step(action)` (Gymnasium API).\n",
    "   - Preprocess the next frame.\n",
    "   - Transform reward using `transform_reward()`.\n",
    "   - Store the transition in the replay buffer.\n",
    "   - Call `agent.train_step()` to update the Q-network (once enough data exists).\n",
    "3. If `done` (episode is over), log total reward and start a new episode.\n",
    "4. Periodically plot and save training curves.\n",
    "\n",
    "We will limit the number of episodes here for demonstration, but the full training often requires many more episodes for good performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2a4c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "NUM_EPISODES = 20         # keep small for testing; increase for real training\n",
    "MAX_STEPS_PER_EPISODE = 50\n",
    "\n",
    "episode_rewards = []      # track total reward per episode\n",
    "losses = []               # track training loss when available\n",
    "\n",
    "for episode in range(1, NUM_EPISODES + 1):\n",
    "    obs, info = env.reset()\n",
    "    state = process_frame(obs, IMAGE_SHAPE)  # (1, H, W, 1)\n",
    "\n",
    "    total_reward = 0.0\n",
    "    episode_loss = []\n",
    "\n",
    "    for t in range(MAX_STEPS_PER_EPISODE):\n",
    "        # 1. Select action using epsilon-greedy\n",
    "        action = agent.select_action(state)\n",
    "\n",
    "        # 2. Step the environment (new Gymnasium API)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # 3. Transform reward (e.g., sign)\n",
    "        reward = transform_reward(reward)\n",
    "\n",
    "        # 4. Preprocess next observation\n",
    "        next_state = process_frame(next_obs, IMAGE_SHAPE)\n",
    "\n",
    "        # 5. Store transition in replay buffer\n",
    "        agent.store_transition(state, action, reward, next_state, done)\n",
    "\n",
    "        # 6. Perform one training step (if enough data)\n",
    "        loss = agent.train_step()\n",
    "        if loss is not None:\n",
    "            episode_loss.append(loss)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    mean_loss = np.mean(episode_loss) if episode_loss else np.nan\n",
    "    losses.append(mean_loss)\n",
    "\n",
    "    print(\n",
    "        f\"Episode {episode}/{NUM_EPISODES} \"\n",
    "        f\"| Total Reward: {total_reward:.2f} \"\n",
    "        f\"| Mean Loss: {mean_loss:.4f} \"\n",
    "        f\"| Replay Buffer Size: {len(agent.replay_buffer)}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36afbb36",
   "metadata": {},
   "source": [
    "## 6. Results & Visualizations\n",
    "\n",
    "We visualize:\n",
    "\n",
    "- Episode total reward vs episode index.\n",
    "- (Optionally) Mean loss per episode.\n",
    "\n",
    "This helps us see whether the agent is improving over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0f334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot episode rewards\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(episode_rewards, marker='o')\n",
    "plt.title(\"Episode Total Reward over Training\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot mean loss per episode (may be noisy or NaN at start)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, marker='x')\n",
    "plt.title(\"Mean Training Loss per Episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86727ba",
   "metadata": {},
   "source": [
    "### Optional: Save Trained Model\n",
    "\n",
    "We can save the trained online Q-network in the newer `.keras` format (not `.h5`),\n",
    "so it can be reloaded later for evaluation or demonstration.\n",
    "\n",
    "This step is **optional** and not required by the assignment,\n",
    "but useful for re-running the agent without retraining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eae374",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "model_path = os.path.join(\"models\", \"pong_dqn_model.keras\")\n",
    "\n",
    "agent.save(model_path)\n",
    "print(\"Model saved to:\", model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54303738",
   "metadata": {},
   "source": [
    "## 7. Reflections: Exploration vs Exploitation & Connection to Casino Exercise\n",
    "\n",
    "In the **Multi-Armed Bandit workshop (Exercise 1)**, we saw:\n",
    "\n",
    "- ε-greedy with different ε values (e.g., 0.1, 0.01).\n",
    "- Stationary vs non-stationary reward distributions.\n",
    "- The impact of constant step size (α) in non-stationary settings.\n",
    "\n",
    "In this **Pong DQN assignment**, we see similar ideas:\n",
    "\n",
    "- The DQN agent uses **ε-greedy**:\n",
    "  - Starts with ε = 1.0 (pure exploration).\n",
    "  - Gradually decays to ε = 0.1 (more exploitation).\n",
    "- When ε is too high → the agent behaves randomly (like constantly exploring in the casino).\n",
    "- When ε is too low too early → the agent may get stuck in a suboptimal policy (greedy choice based on limited experience).\n",
    "\n",
    "The **experience replay buffer** also plays a role similar to having a\n",
    "larger sample of past pulls in the casino:\n",
    "- It reduces variance and stabilizes learning.\n",
    "- It lets the agent \"remember\" many different states and rewards.\n",
    "\n",
    "Finally, the **target network** acts as a slowly-changing reference,\n",
    "reducing the risk of chasing non-stationary targets, similar to how\n",
    "using a stable estimate of the true arm values helps in the casino.\n",
    "\n",
    "You can further explore:\n",
    "- Different ε decay schedules (slower/faster).\n",
    "- Different replay buffer sizes.\n",
    "- Different network architectures.\n",
    "\n",
    "These experiments strengthen the understanding of the **exploration–exploitation trade-off**\n",
    "across both the bandit setting and the full RL setting with Pong.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
